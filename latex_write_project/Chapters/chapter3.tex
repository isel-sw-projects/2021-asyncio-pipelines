

\lstset{basicstyle=\small\ttfamily, breaklines=true, frame=single}
\chapter{Benchmarking Use Case Algorithms and Benchmark Setup}
\label{cha:benchmarking_use_case_algorithms}

In order to have metrics to enable the comparison between some of the different technologies we discussed so far, we decided to make software implementations in three programming languages of two key algorithms. 

Each selected to simulate two distinct data processing scenarios. These are the "Count each word occurrence" or "Group words" and "Find the biggest word", both of which offer valuable insights into different aspects of data processing.

The "Group Word" operation it's a memory-intensive task. This operation involves analyzing a dataset to identify and count each word occurrences that fall within a specified size range. It uses data structures, such as dictionaries, for in runtime data storage. These nuances make the "Group Word" operation a curious test case for evaluating the capabilities of asynchronous processing in combination with a runtime memory demanding scenario.

In contrast, the task of finding the largest word, though less complex, is significant for its simplicity. This operation involves scanning a dataset to find the single longest word, a process that, while straightforward, is critical for understanding the performance of asynchronous processing in more basic computational tasks. It serves as a benchmark for evaluating the efficiency of simpler algorithms and their implementation across different programming languages and environments.

For this work, were chosen these two algorithm as simplistic samples for a simplistic benchmark scenario, but, for future work, can be added a more vast algorithmic variety.

Bellow, we can find the pseudocode of these algorithms, that may help to understand how they function:


\section{Overview of Algorithms}
\label{sec:algorithm_overview}


\subsection*{Pseudocode for the Group Word Operation}
\begin{lstlisting}[language={}, caption={Pseudocode for GroupWords function}, label={lst:groupwords}]
FUNCTION GroupWords(folder, minLength, maxLength)
    Create an empty map 'wordMap'
    FOR each file in 'folder' DO
        Skip the first 14 lines of the file (These are typically metadata in Gutenberg project files)
        FOR each remaining line in 'file' DO
            IF the line contains "*** END OF" THEN
                Break (This is the end of the actual content in Gutenberg project files)
            END IF
            FOR each word in 'line' DO
                IF length of 'word' is between 'minLength' and 'maxLength' THEN
                    Increment the count of 'word' in 'wordMap'
                END IF
            END FOR
        END FOR
    END FOR
    RETURN 'wordMap'
END FUNCTION
\end{lstlisting}

\clearpage

\subsection*{Pseudocode for the Find Biggest Word Operation}
\begin{lstlisting}[language={}, caption={Pseudocode for FindLargestWord function}, label={lst:findlargestword}]
FUNCTION FindLargestWord(folder)
    Set 'largestWord' as an empty string
    FOR each file in 'folder' DO
        Skip the first 14 lines of the file (These are typically metadata in Gutenberg project files)
        FOR each remaining line in 'file' DO
            IF the line contains "*** END OF" THEN
                Break (This is the end of the actual content in Gutenberg project files)
            END IF
            FOR each word in 'line' DO
                IF length of 'word' is greater than length of 'largestWord' THEN
                    Set 'largestWord' as 'word'
                END IF
            END FOR
        END FOR
    END FOR
    RETURN 'largestWord'
END FUNCTION
\end{lstlisting}

To benchmark each algorithm, was chosen to make a baseline implementations, which the code resembles the syntax complexity close to the pseudocode already shown and, on the other side, was used pipeline chains, using fluent API provided, for example, by RxJava or C\# in AsyncEnumerable.

The idea behind this, was to explore readability vs performance in the use of these technologies.


\section{Pipelines}
\label{sec:pipeline_implementations}

During the implementation of the benchmarking algorithms, was made a conscious effort to keep the operation pipelines as similar as possible across the different technologies for each algorithm. This endeavor aimed to create a fair and representative evaluation of the behaviors of each technology. By maintaining consistency in pipeline operations, we can more accurately attribute performance differences to the underlying technology, rather than variations in the implemented code. This approach brings us closer to a true comparison of how each technology handles the challenges of asynchronous I/O data retrieval and processing.

In certain instances, particularly with Java, there was a need to incorporate external libraries for non-blocking asynchronous file retrieval. Because, while some environments have native functions that already enable non-blocking IO operations to retrieve data from files, other, like in JAVA, was needed to use external libraries.


\subsection{Understanding General Pipeline Operations}
\label{sec:understanding_pipeline_operations}

In the realm of functional programming and modern software development, several key operations are fundamental to process data collections and data flows. These operations, while conceptually similar across different languages, may have different idioms. Below there are explained several key operations that are usually used as operation in several fluent APIÂ´s to construct operation pipelines.

\subsection*{Sort}
The sort operation arranges elements of a collection in a specific order, typically ascending or descending. It's crucial for organizing data in a meaningful way.

\textbf{In C\#}: Implemented as \texttt{.OrderBy} (ascending) and \texttt{.OrderByDescending} (descending). \\
\textbf{In Java}: Executed using \texttt{.sorted()} with comparators for custom sorting. \\
\textbf{In JavaScript}: Uses \texttt{.sort()}, which can take a comparison function for custom sorting.

\subsection*{Distinct}
The distinct operation removes duplicate elements from a collection, ensuring each element is unique.

\textbf{In C\#}: Available as \texttt{.Distinct()}. \\
\textbf{In Java}: Achieved using \texttt{.distinct()} in the Stream API. \\
\textbf{In JavaScript}: Typically done using \texttt{new Set([...array])} to remove duplicates.

\subsection*{GroupBy}
The \texttt{groupBy} operation groups elements of a collection based on a specified key. This is useful for categorizing data.

\textbf{In C\#}: Done using \texttt{.GroupBy}. \\
\textbf{In Java}: Performed using \texttt{.collect(Collectors.groupingBy())}. \\
\textbf{In JavaScript}: Often implemented with \texttt{Array.prototype.reduce()} for custom grouping logic.

\subsection*{Concat and Union}
Concat combines two sequences end-to-end, while union merges two sequences and removes duplicates.

\textbf{In C\#}: \texttt{Concat} for concatenation and \texttt{Union} for union operations. \\
\textbf{In Java}: \texttt{.concat} for concatenation and using \texttt{.distinct()} after \texttt{.concat} for union. \\
\textbf{In JavaScript}: \texttt{.concat()} for concatenation; a combination of \texttt{.concat()} and \texttt{new Set()} for union.

\subsection*{Any and All}
Any checks if any elements in the collection satisfy a condition, while All checks if all elements meet a condition.

\textbf{In C\#}: Implemented as \texttt{.Any()} and \texttt{.All()}. \\
\textbf{In Java}: \texttt{.anyMatch()} and \texttt{.allMatch()} in the Stream API. \\
\textbf{In JavaScript}: \texttt{.some()} for any, and \texttt{.every()} for all.

\subsection*{Count and Sum}
Count returns the number of elements, and Sum calculates the total of the numeric elements in a collection.

\textbf{In C\#}: \texttt{.Count()} for counting and \texttt{.Sum()} for summing. \\
\textbf{In Java}: \texttt{.count()} and using \texttt{.mapToDouble()} followed by \texttt{.sum()} for summing. \\
\textbf{In JavaScript}: \texttt{.length} for count and \texttt{.reduce()} for summing.

\subsection*{Flat and FlatMap}
Flat merges all sub-array elements into a new array, while FlatMap first applies a mapping function to each element and then flattens the result into a new array.

\textbf{In C\#}: \texttt{.SelectMany()} is a close equivalent to FlatMap. \\
\textbf{In Java}: \texttt{.flatMap()} in the Stream API. \\
\textbf{In JavaScript}: \texttt{.flat()} for flat and \texttt{.flatMap()} for flatMap.

\subsection*{Zip}
Zip combines elements of two collections into pairs or tuples, often based on their position in the collection.

\textbf{In C\#}: Available as \texttt{.Zip()}. \\
\textbf{In Java}: Achieved using a combination of \texttt{.stream()} and \texttt{.map()} with a custom zipper function. \\
\textbf{In JavaScript}: Implemented manually using methods like \texttt{.map()} with additional logic for pairing.

\subsection*{Filter}
The filter operation evaluates each element against a predicate (a true/false function) and includes only those elements that satisfy the predicate.

\textbf{In C\#}: Implemented as \texttt{.Where}. \\
\textbf{In Java}: Known as \texttt{.filter}. \\
\textbf{In JavaScript}: Executed using \texttt{.filter()}.

\subsection*{ForEach}
The ForEach operation applies a given action to each element in a collection. It's typically used for invoking side effects or operations on each element.

\textbf{In C\#}: Available as \texttt{.ForEach} in the List<T> class or via looping constructs like \texttt{foreach}. \\
\textbf{In Java}: Implemented using \texttt{.forEach} in the Stream API or via looping constructs like for-each. \\
\textbf{In JavaScript}: Executed using \texttt{.forEach()}, which applies a function to each element of an array or array-like objects.

\subsection*{Map}
The map operation applies a function to each element in a collection, transforming them into a new form. It's a cornerstone of functional programming, enabling easy data transformation.

\textbf{In C\#}: Named as \texttt{.Select}. \\
\textbf{In Java}: Referred to as \texttt{.map}. \\
\textbf{In JavaScript}: Uses \texttt{.map()}, which applies a specified function to each element of an array, returning a new array with the transformed elements.

The consistent use of these operations across different languages underscores a universal shift towards more declarative and expressive programming styles, where the focus is on what needs to be done rather than how to do it. This approach not only enhances code readability and maintainability but also allows for more concise and functional solutions to common programming tasks.

\section{Pipeline Examples}
Bellow, we can see the pipeline versions of our key algorithms in C\# and Java against their pseudocode representations already shown. The pseudocode, which is closer to a baseline implementation, serves as a point of reference to appreciate the enhanced readability and conciseness offered by pipelines. By comparing these implementations, we can better understand how pipelines abstract complexity and streamline data processing tasks.

\clearpage

\lstset{basicstyle=\footnotesize\ttfamily} % Adjusts the font size for code examples

\subsection{C\# Pipeline Example}

\begin{lstlisting}[language={[Sharp]C}, caption={C\# Pipeline for Parsing Distinct Words into a Dictionary}]
private Task<string> parseFileDistinctWordsIntoDictionary(string filename)
{
    return FileUtils.getLinesAsyncEnum(filename)
        .Where(line => line.Length != 0)   
        .Skip(14)                          
        .TakeWhile(line => !line.Contains("*** END OF ")) 
        .Select(line => Regex.Replace(line, "[^a-zA-Z0-9 -]+", "", RegexOptions.Compiled)
            .Split(' ')
            .Max(arr => arr)) // Find the longest word
        .AggregateAsync(string.Empty, (biggest, current) => current.Length > biggest.Length ? current : biggest) 
        .AsTask();
}
\end{lstlisting}

\subsection{Java Pipeline Example}
\begin{lstlisting}[language=Java, caption={Java Pipeline for Finding the Largest Word in Files}]
Files.list(Paths.get("path/to/directory"))
    .filter(Files::isRegularFile)   // Process only regular files
    .map(file -> EXEC.submit(() -> biggestWord(file))) // Submit a task for each file
    .collect(toList())             // Collect futures into a list
    .stream()
    .map(future -> waitForFuture(future).get()) // Process each future
    .reduce((biggest, curr) -> curr.length() > biggest.length() ? curr : biggest) // Reduce to find the largest word
    .get();
\end{lstlisting}

\lstset{basicstyle=\normalsize\ttfamily} % Resets the font size for subsequent listings

These C\# and Java implementations showcase the power and elegance of pipeline processing in handling tasks that would otherwise require more verbose and complex code.
