

\lstset{basicstyle=\small\ttfamily, breaklines=true, frame=single}
\chapter{Benchmarking Use Case Algorithms and Benchmark Setup}
\label{cha:benchmarking_use_case_algorithms}

In this chapter, we embark on an in-depth exploration of the benchmarking process for two significant algorithms. These algorithms have been carefully selected to represent complex, real-world tasks that require efficient asynchronous data processing. Our focus will be on understanding how different strategies and implementations influence the performance and effectiveness of these algorithms.

\section{Overview of Algorithms}
\label{sec:algorithm_overview}

The benchmarking study revolves around two principal algorithms, each presenting unique challenges in asynchronous data processing:


\begin{enumerate}
\item Finding the largest word in a set of files
\item Grouping of words based on their sizes, known as the "group words" operation
\end{enumerate}

The "group word" operation identifies words within a specified size range in a dataset. The algorithm then returns a collection of these words, along with a count of their occurrences, and is particularly designed to output the most recurring words within the specified range. This operation presents an intriguing computational challenge as it requires efficient data retrieval, processing, and frequency analysis.

Conversely, finding the largest word in a set of files, although seemingly simple, becomes a non-trivial task when considering vast amounts of data.

The data used in these tests is a collection of text files from Project Gutenberg, a large digital library of thousands of free eBooks. Project Gutenberg offers a wide variety of books in different languages, and for this experiment, we used a random sample of hundreds of books, providing a diverse and challenging dataset for our implementations.

To give a better idea of the operations, we present the pseudocode for each operation:


For the "group words" operation:



\begin{lstlisting}[language={}, caption={Pseudocode for GroupWords function}, label={lst:groupwords}]
FUNCTION GroupWords(folder, minLength, maxLength)
    Create an empty map 'wordMap'
    FOR each file in 'folder' DO
        Skip the first 14 lines of the file (These are typically metadata in Gutenberg project files)
        FOR each remaining line in 'file' DO
            IF the line contains "*** END OF" THEN
                Break (This is the end of the actual content in Gutenberg project files)
            END IF
            FOR each word in 'line' DO
                IF length of 'word' is between 'minLength' and 'maxLength' THEN
                    Increment the count of 'word' in 'wordMap'
                END IF
            END FOR
        END FOR
    END FOR
    RETURN 'wordMap'
END FUNCTION
\end{lstlisting}

\begin{lstlisting}[language={}, caption={Pseudocode for FindLargestWord function}, label={lst:findlargestword}]
FUNCTION FindLargestWord(folder)
    Set 'largestWord' as an empty string
    FOR each file in 'folder' DO
        Skip the first 14 lines of the file (These are typically metadata in Gutenberg project files)
        FOR each remaining line in 'file' DO
            IF the line contains "*** END OF" THEN
                Break (This is the end of the actual content in Gutenberg project files)
            END IF
            FOR each word in 'line' DO
                IF length of 'word' is greater than length of 'largestWord' THEN
                    Set 'largestWord' as 'word'
                END IF
            END FOR
        END FOR
    END FOR
    RETURN 'largestWord'
END FUNCTION
\end{lstlisting}


During the implementations, a conscious effort was made to keep the operation pipelines as similar as possible across the different technologies for each algorithm. This endeavor aimed to create a fair and representative evaluation of the behaviors of each technology. By maintaining consistency in pipeline operations, we can more accurately attribute performance differences to the underlying technology, rather than variations in the implemented code. This approach brings us closer to a true comparison of how each technology handles the challenges of asynchronous I/O data retrieval and processing.

In certain instances, particularly with Java, there was a need to incorporate external libraries for non-blocking asynchronous file retrieval. This requirement highlights the varying degrees of native support for these operations across programming environments. Some environments have native functions, while others rely heavily on third-party solutions. This aspect of the project mirrors the challenges often faced in real-world environments, where the necessity to adapt and find suitable solutions is a common part of the development process.

The implementations differ in the specific programming languages, libraries, and technologies they use. This allows us to evaluate the relative strengths and weaknesses of each approach and provides valuable insights into how these factors can affect performance.

After presenting and discussing each implementation and its results, we will be able to directly compare them. This will enable us to draw meaningful conclusions about the performance of the strategies and technologies when applied to the same task under the same conditions. Specifically, we are interested in how the performance of a given strategy or technology can vary across different programming frameworks when tasked with finding the largest word and executing the group word operation.


\section{Pipeline Implementations}
\label{sec:pipeline_implementations}

To illustrate the advantages of pipeline implementations in modern programming, this section contrasts the pipeline versions of our key algorithms in C\# and Java against their pseudocode representations. The pseudocode, which is closer to a baseline implementation, serves as a point of reference to appreciate the enhanced readability and conciseness offered by pipelines. By comparing these implementations, we can better understand how pipelines abstract complexity and streamline data processing tasks.

\section{Understanding General Pipeline Operations}
\label{sec:understanding_pipeline_operations}

In the realm of functional programming and modern software development, several key operations are fundamental to processing collections of data. These operations, while conceptually similar across different languages, may have different names or implementations. Below is a general explanation of each operation, along with their equivalents in C\# and Java.

\subsection{Map}
\textbf{Description:} The `map` operation applies a function to each element in a collection, transforming them into a new form. It's a cornerstone of functional programming, enabling easy data transformation.
\textbf{In C\#}: Named as `.Select`.
\textbf{In Java}: Referred to as `.map`.

\subsection{Filter}
\textbf{Description:} The `filter` operation evaluates each element against a predicate (a true/false function) and includes only those elements that satisfy the predicate.
\textbf{In C\#}: Implemented as `.Where`.
\textbf{In Java}: Known as `.filter`.

\subsection{Reduce}
\textbf{Description:} The `reduce` operation combines all elements in a collection to produce a single aggregated result, such as a sum or maximum value.
\textbf{In C\#}: Known as `.Aggregate`.
\textbf{In Java}: Referred to as `.reduce`.

\subsection{Skip and TakeWhile}
\textbf{Description:} `Skip` omits a specified number of elements from the start of the collection, while `TakeWhile` continues taking elements until a condition is no longer met.
\textbf{In C\# and Java}: Both operations have the same names, `.Skip` and `.TakeWhile`.

\subsection{ForEach}
\textbf{Description:} `ForEach` applies a given action to each element in the collection. It's typically used for invoking side effects.
\textbf{In C\#}: Available as `.ForEach` in the `List<T>` class or via looping constructs.
\textbf{In Java}: Available as `.forEach` in the Stream API or via looping constructs.



\section{Pipeline Examples}
The following examples in C\# and Java demonstrate the use of pipelines for finding the largest word in a set of files. These examples highlight how pipelines streamline complex data processing by chaining a series of operations.

\lstset{basicstyle=\footnotesize\ttfamily} % Adjusts the font size for code examples

\subsection{C\# Pipeline Example}
\begin{lstlisting}[language={[Sharp]C}, caption={C\# Pipeline for Parsing Distinct Words into a Dictionary}]
private Task<string> parseFileDistinctWordsIntoDictionary(string filename)
{
    return FileUtils.getLinesAsyncEnum(filename)
        .Where(line => line.Length != 0)   // Filter out empty lines
        .Skip(14)                          // Skip Gutenberg project header
        .TakeWhile(line => !line.Contains("*** END OF ")) // Process until a certain string
        .Select(line => Regex.Replace(line, "[^a-zA-Z0-9 -]+", "", RegexOptions.Compiled)
                              .Split(' ')
                              .Max(arr => arr)) // Find the longest word
        .AggregateAsync(string.Empty, (biggest, current) => current.Length > biggest.Length ? current : biggest) // Aggregate to find the longest word
        .AsTask();
}
\end{lstlisting}

\subsection{Java Pipeline Example}
\begin{lstlisting}[language=Java, caption={Java Pipeline for Finding the Largest Word in Files}]
Files.list(Paths.get("path/to/directory"))
    .filter(Files::isRegularFile)   // Process only regular files
    .map(file -> EXEC.submit(() -> biggestWord(file))) // Submit a task for each file
    .collect(toList())             // Collect futures into a list
    .stream()
    .map(future -> waitForFuture(future).get()) // Process each future
    .reduce((biggest, curr) -> curr.length() > biggest.length() ? curr : biggest) // Reduce to find the largest word
    .get();
\end{lstlisting}

\lstset{basicstyle=\normalsize\ttfamily} % Resets the font size for subsequent listings

These C\# and Java implementations showcase the power and elegance of pipeline processing in handling tasks that would otherwise require more verbose and complex code.
